[
  {
    "id": "task_001",
    "title": "Enhanced Job Scraping with Browser Rendering and Authentication Handling",
    "description": "Upgrade the existing web scraper in `src/lib/crawl.ts` to robustly handle modern, JavaScript-heavy job sites. The scraper must be able to manage sites that require authentication by using stored credentials. Implement a stateful crawling mechanism using Durable Objects (`SiteCrawler`) to manage scraping sessions on a per-site basis, including handling cookies and session tokens.",
    "subtasks": [
      "Integrate `puppeteer` with the `MYBROWSER` binding to control a headless browser for scraping.",
      "Develop a secure method for storing and retrieving site credentials (e.g., from KV or a secure secret management system).",
      "Implement logic within the `SiteCrawler` Durable Object to log in to sites and maintain an authenticated session.",
      "Add error handling for common scraping challenges like CAPTCHAs, dynamic content loading, and anti-bot measures.",
      "Create a snapshot of the job posting's fully rendered HTML and a screenshot, storing them in R2 for archival and analysis."
    ],
    "status": "pending"
  },
  {
    "id": "task_002",
    "title": "Automated Resume and Cover Letter Generation",
    "description": "Create a new module for generating tailored resumes and cover letters. This module will take a scraped job description and a user's profile from the D1 database, then use Workers AI to generate documents that highlight the most relevant skills and experiences. The output should be structured and easily convertible to formats like PDF or DOCX.",
    "subtasks": [
      "Design and implement a D1 database schema for storing user profiles, including work experience, skills, education, and projects.",
      "Develop a `generateResume` function that uses a sophisticated prompt and a JSON schema to instruct an LLM to create a tailored professional summary, experience bullet points, and skills list.",
      "Implement the existing `cover_letter_agent.ts` logic into a reusable module that can be called from the main worker.",
      "Create a mechanism to convert the generated JSON content into a formatted HTML preview.",
      "Integrate Browser Rendering (`MYBROWSER`) to convert the final HTML preview into a downloadable PDF."
    ],
    "status": "pending"
  },
  {
    "id": "task_003",
    "title": "AI-Powered Job Insight Engine",
    "description": "Develop a feature that analyzes a scraped job description to provide the applicant with meaningful insights. This engine will use Workers AI to identify key skills, infer company culture, suggest areas of the resume to emphasize, and generate potential interview questions.",
    "subtasks": [
      "Create a new AI-powered function `analyzeJobDescription(jobText)`.",
      "Develop a prompt that instructs the LLM to perform multiple analyses: skills extraction, sentiment/tone analysis, and question generation.",
      "Use a structured JSON schema with `guided_json` to ensure the AI's output is consistent and easily parsable.",
      "Store the generated insights in the D1 database, linked to the specific job application.",
      "Use Vectorize to create embeddings from the extracted skills and match them against the user's skills from their profile to identify strengths and gaps."
    ],
    "status": "pending"
  },
  {
    "id": "task_004",
    "title": "User Profile Management API",
    "description": "Build a secure API endpoint for creating, retrieving, updating, and deleting a user's professional profile in the D1 database. This API will be used by a future frontend to manage the applicant's data.",
    "subtasks": [
      "Define API routes for CRUD operations on the user profile (e.g., `POST /api/profile`, `GET /api/profile`, `PUT /api/profile`).",
      "Implement the database logic in `src/lib/storage.ts` for each CRUD operation on the `applicant_profiles` and related tables.",
      "Ensure proper authentication and authorization for all profile-related endpoints.",
      "Implement input validation to ensure data integrity for all incoming profile data."
    ],
    "status": "pending"
  }
]
